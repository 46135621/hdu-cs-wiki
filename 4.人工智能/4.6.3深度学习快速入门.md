# 深度学习快速入门

## <strong>刘二大人(Pytorch)</strong>

# [【速成课：人工智能】Ai - [21 集全/中英双语] - Artificial Intelligence_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1P7411r7Dw/?spm_id_from=333.999.0.0&vd_source=2cb6252f9211ae9d29cf1f76f0aea8d7)

Crash course 的课程，适合速成性的了解 AI 的基本方向和内容

# 这是啥？

这是一个快速入门深度学习的途径。

# 课程大概讲了啥？

刘二大人的深度学习是用来给小白快速上手用的。其中介绍了大概的深度学习框架，基本的几种损失函数，激活函数，网络。

课程中用到了 3 个数据集：糖尿病数据集，泰坦尼克号数据集，和最经典的 MINIST 数据集。其中我们只需要用到 MINIST 数据集，其他两个如果有兴趣可以去尝试。我们最快可以在 1 个星期内训练出我们的第一个模型用来识别手写数字，初窥人工智能的门槛。

这个课程最主要的是着重讲解了大致的框架，深度学习的代码就像搭积木一样，当大致的框架有了，剩下的就只剩下往里面塞东西就好了。当我们学习了刘二大人的课程之后，一些基本的任务都可以用这些基本的网络简单解决。

# 学习这系列视频需要哪些前置条件？

## python

基本的一些 python 知识，你可以在本讲义中的 [python 模块](https://gw9u39xwqi.feishu.cn/wiki/wikcn8RxD1oJ4w5BVOIS9QpS4xQ)中进行简单的学习。解决其中的题目大致就可以了，之后遇到不会的只要去 Google 一下，或者去问问 ChatGPT，问问 New Bing。

## pycharm,pytorch,anaconda 等环境配置

你可以在本讲义中的 [Pytorch 安装](https://gw9u39xwqi.feishu.cn/wiki/wikcn6Bpl4ynpMSX4C8LVSYouXf)中找到怎么配置 pytorch，你可以在这里安装 [Pycharm](https://www.jetbrains.com/zh-cn/pycharm/)。

你可以在本讲义中的 [python 安装](https://gw9u39xwqi.feishu.cn/wiki/wikcnN68wBfMYy6xOWhbo2uChsh)中找到 Pycharm 和 anaconda 的安装教学视频

## 一个找乐子的心

如果觉得它好玩的话，就去学吧。

## 前置知识？

要啥前置知识，这就是给你入门用的。如果你不打无准备的仗，你可以简单看看[机器学习快速入门](https://gw9u39xwqi.feishu.cn/wiki/wikcnmBWmI8fZmhynbnHzxrRMMd)。

# 学完课程之后可能出现的问题

通过这个课程虽然我们可以进行快速入门，但经过我个人的入门实践表明，视频中没有告诉你完整的数学推导，你们在那时候也懒得进行公式推导，所以在观看这门教程之后虽然已经会基本的 coding 能力了但是基础并不扎实。

我们不知道每一个给你封装好的函数具体在干什么，不知道经过这个线性层，经过这个卷积操作出来的特征大致对应着什么，它们对我们来说确实变成了一个黑盒。我们只知道：欸，我就这么一写，in_feature 和 out_feature 写对了，程序成功运行了，正确率有 80% 多欸，我已经会深度学习了。

所以在这门课程结束之后建议手写其中的一些封装好的函数，比如一些基础的线性层。尝试画个图，像课程中刘二讲给我们的那样，看看大致的流程，每一层出来的特征大致代表着什么。

# 你还有疑惑？

你可以通过以下方式解决你对于此课程的疑惑：

## CSDN

由于是一款大众的入门视频，所以有很多课代表记了笔记，发了作业答案在 CSDN 上，你可以在课代表这进行课前预习，课后复习，作业抄袭  ：）

## 基础知识的疑惑

如果你对于课程中的一些基本知识比如说梯度下降算法等感到疑惑，你可以移步[机器学习快速入门](https://gw9u39xwqi.feishu.cn/wiki/wikcnmBWmI8fZmhynbnHzxrRMMd)

当然，在这里我会简单的为你讲解一下最基础最关键的算法：梯度下降算法。和怎么快速理解计算机为什么能识别手写数字。

## torch 我还不会呢！

学会一个<strong>庞大并且高度封装</strong>的包并不是一蹴而就的，我们建议从实践开始，比如说自己搭建一个神经网络来实现 MNIST 的分类。在使用这些函数和类的过程中你能更快地掌握它们的方法。

# 关于梯度下降算法：

### 损失

![](static/boxcnRbeabbEppeHlM39UwqJSJc.png)

首先我们需要有一个损失函数
$$
F（x），x=true-predict
$$


这样通过一个函数我们就得到了一个具体的数值，这个数值的意义是：现在的输入数据经过一个拟合函数处理后得到的结果和真实结果的差距，梯度下降算法就是根据这个为基础进行对拟合函数中参数的优化。

### 梯度下降

![](static/boxcnMuwaG2okodvywzbxX138Re.png)

假设损失函数为
$$
y=x^2
$$

,梯度下降的目的是快速找到导数为 0 的位置（附近）

![](static/boxcn83M9AW6xDm5pBIqmZEC6Kf.png)

![](static/boxcneVFa131Lb9xDMCsIMI9fcc.png)

以此类推，我们最后的 w 在 0 的附近反复横跳，最后最接近目标函数的权重 w 就是 0。

### 简单理解

你可以简单这样理解：游戏中你在靶场练狙击枪，你用 4-8 倍镜瞄了 400 米的靶子，真实值就是靶心。你开了一枪后发现落在靶心上方，于是你根据距离靶心的远近，你的大脑开始计算优化下次瞄的位置，如果你往上面偏了很多，你就会将瞄点往下移动很多，如果往上偏了一点点，你就会将瞄点往下移动一点点。

移动的途中可能出现移动的过多的情况，从上偏变成下偏了，这就是如果学习率过大会出现的问题。

总而言之，你打狙击枪脑子怎么分析的，梯度下降算法就是怎么算的。当然由于它是电脑没有灵活的机动性，他的权重只能逐渐改变。

# 关于 MINIST

![](static/boxcnxdyWA6Sj82kNxMlQ1b9hDg.png)

这个数据集可以说是最最经典的数据集了，里面有 0-9 这 10 个数字的手写图片和标注，正确率最高已经到了 99.7%.

# 接下来干什么？

- <strong>我想学 CV !!!!!!</strong>

你可以在 CV 模块中找到[经典网络](https://gw9u39xwqi.feishu.cn/wiki/wikcnnLjMBAKyqH5WKK3jXa8xdc) ，这里是一些最最经典的论文，我们推荐你阅读它们的原文并且复现它们的代码，这可以同时锻炼你的<strong>coding 能力和论文阅读能力</strong>，在阅读前，请参见[如何读论文](https://gw9u39xwqi.feishu.cn/wiki/wikcnRXiOz6FuOSGjPxmBgCNvcd) 。本模块的撰写者<strong>SRT 社团</strong>主要从事 CV 方向的研究，遇到问题欢迎与我们交流。（你都完成这些了不至于找不到我们的联系方式吧~）<strong>如果你读完了经典网络模块，你可以在它的最后找到接下来的学习路线~</strong>

- <strong>我想做</strong><strong>NLP</strong><strong> !!!!!!</strong>

NLP 研究方向庞大且复杂，若直接从 GPT 系列开始不免有些过于困难。我们建议你从了解 NLP 的任务开始，在有足够的基础后开始学习 RNN，LSTM 基准方法后向 [Transformer](https://gw9u39xwqi.feishu.cn/wiki/wikcnPNyq3BTIYKT6LFlViyTzNe) 进发 ，这个方法广泛运用在几乎所有深度学习领域，尤其是 NLP 的前沿研究已经无法离开 Transformer 了 hhhh。这个模块中我们也加入了一些 Transformer 的改进工作，包括 NLP，CV，和多模态

- <strong>如果你想做多模态，对比学习等</strong>，请同时了解 CV 和 NLP 模块。这将是你后续知识的基础。多模态我们没有完善的讲义推出，对比学习可以参见[对比学习](https://gw9u39xwqi.feishu.cn/wiki/wikcngR1r66tof102Aof4WywlXf) 。这是撰写者之一的论文阅读笔记，不保证准确性与理解是否准确，可以作为论文阅读路线图来参考~
